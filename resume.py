# YOLO v11 í•™ìŠµ ì¬ê°œ ë° ì¶”ê°€ í•™ìŠµ (ì™„ì „íŒ)

import torch
import os
from ultralytics import YOLO
import glob
import yaml

# ê¸°ë³¸ ì„¤ì •
DATASET_ROOT = "./dataset"
PROJECT_NAME = "beekeeping"
EXPERIMENT_NAME = "dreambee"
CLASS_NAMES = ['egg', 'lava', 'pupa', 'bee', 'queen', 'Chalkbrood']

def create_or_check_dataset_yaml(dataset_root, classes, yaml_path="dataset.yaml"):
    """ë°ì´í„°ì…‹ YAML íŒŒì¼ ìƒì„± ë˜ëŠ” í™•ì¸"""
    
    # YAML íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ê²€ì¦
    if os.path.exists(yaml_path):
        print(f"âœ… ê¸°ì¡´ dataset.yaml íŒŒì¼ ë°œê²¬: {yaml_path}")
        try:
            with open(yaml_path, 'r', encoding='utf-8') as f:
                existing_config = yaml.safe_load(f)
            
            # ê¸°ì¡´ ì„¤ì • ì¶œë ¥
            print(f"   - ë°ì´í„°ì…‹ ê²½ë¡œ: {existing_config.get('path')}")
            print(f"   - í´ë˜ìŠ¤ ìˆ˜: {existing_config.get('nc')}")
            print(f"   - í´ë˜ìŠ¤ ì´ë¦„: {list(existing_config.get('names', {}).values())}")
            
            # í´ë˜ìŠ¤ ìˆ˜ ì¼ì¹˜ í™•ì¸
            if existing_config.get('nc') != len(classes):
                print(f"âš ï¸  í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜: ê¸°ì¡´ {existing_config.get('nc')} vs í˜„ì¬ {len(classes)}")
                
            return yaml_path
            
        except Exception as e:
            print(f"âŒ YAML íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            print("ìƒˆë¡œìš´ YAML íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # ìƒˆë¡œìš´ YAML íŒŒì¼ ìƒì„±
    dataset_yaml = f"""# Train/val/test sets
path: {os.path.abspath(dataset_root)}  # dataset root dir
train: images/train  # train images (relative to 'path')
val: images/val  # val images (relative to 'path')

# Classes
names:
"""
    
    # í´ë˜ìŠ¤ ì¶”ê°€
    for i, class_name in enumerate(classes):
        dataset_yaml += f"  {i}: {class_name}\n"
    
    dataset_yaml += f"\nnc: {len(classes)}  # number of classes"
    
    # YAML íŒŒì¼ ì €ì¥
    with open(yaml_path, 'w', encoding='utf-8') as f:
        f.write(dataset_yaml)
    
    print(f"âœ… Dataset YAML íŒŒì¼ ìƒì„± ì™„ë£Œ: {yaml_path}")
    return yaml_path

def validate_dataset_structure(dataset_root):
    """ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦"""
    print(f"=== ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦: {dataset_root} ===")
    
    if not os.path.exists(dataset_root):
        print(f"âŒ ë°ì´í„°ì…‹ ë£¨íŠ¸ í´ë” ì—†ìŒ: {dataset_root}")
        return False
    
    # í•„ìˆ˜ í´ë” í™•ì¸
    required_folders = [
        'images/train',
        'images/val', 
        'labels/train',
        'labels/val'
    ]
    
    missing_folders = []
    for folder in required_folders:
        full_path = os.path.join(dataset_root, folder)
        if not os.path.exists(full_path):
            missing_folders.append(folder)
        else:
            # íŒŒì¼ ê°œìˆ˜ í™•ì¸
            file_count = len(glob.glob(os.path.join(full_path, '*')))
            print(f"âœ… {folder}: {file_count}ê°œ íŒŒì¼")
    
    if missing_folders:
        print(f"âŒ ëˆ„ë½ëœ í´ë”: {missing_folders}")
        return False
    
    # ì´ë¯¸ì§€ì™€ ë¼ë²¨ ë§¤ì¹­ í™•ì¸
    train_images = set(os.path.splitext(os.path.basename(f))[0] 
                      for f in glob.glob(os.path.join(dataset_root, 'images/train/*')))
    train_labels = set(os.path.splitext(os.path.basename(f))[0] 
                      for f in glob.glob(os.path.join(dataset_root, 'labels/train/*.txt')))
    
    val_images = set(os.path.splitext(os.path.basename(f))[0] 
                    for f in glob.glob(os.path.join(dataset_root, 'images/val/*')))
    val_labels = set(os.path.splitext(os.path.basename(f))[0] 
                    for f in glob.glob(os.path.join(dataset_root, 'labels/val/*.txt')))
    
    # ë§¤ì¹­ ìƒíƒœ í™•ì¸
    train_missing_labels = train_images - train_labels
    train_missing_images = train_labels - train_images
    val_missing_labels = val_images - val_labels
    val_missing_images = val_labels - val_images
    
    if train_missing_labels:
        print(f"âš ï¸  í›ˆë ¨ ì„¸íŠ¸ ë¼ë²¨ ëˆ„ë½: {len(train_missing_labels)}ê°œ")
    if train_missing_images:
        print(f"âš ï¸  í›ˆë ¨ ì„¸íŠ¸ ì´ë¯¸ì§€ ëˆ„ë½: {len(train_missing_images)}ê°œ")
    if val_missing_labels:
        print(f"âš ï¸  ê²€ì¦ ì„¸íŠ¸ ë¼ë²¨ ëˆ„ë½: {len(val_missing_labels)}ê°œ")
    if val_missing_images:
        print(f"âš ï¸  ê²€ì¦ ì„¸íŠ¸ ì´ë¯¸ì§€ ëˆ„ë½: {len(val_missing_images)}ê°œ")
    
    return True

def check_existing_models():
    """ê¸°ì¡´ì— ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤ê³¼ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ í™•ì¸"""
    print("=== ê¸°ì¡´ ëª¨ë¸ í™•ì¸ ===")
    
    # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë“¤ í™•ì¸
    pretrained_models = ['yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt', 'yolo11l.pt', 'yolo11x.pt']
    for model in pretrained_models:
        if os.path.exists(model):
            size = os.path.getsize(model) / (1024*1024)  # MB
            print(f"âœ… {model} ì¡´ì¬ ({size:.1f}MB)")
        else:
            print(f"âŒ {model} ì—†ìŒ")
    
    # í•™ìŠµëœ ëª¨ë¸ë“¤ í™•ì¸
    results_path = os.path.join(PROJECT_NAME, EXPERIMENT_NAME)
    if os.path.exists(results_path):
        weights_path = os.path.join(results_path, 'weights')
        if os.path.exists(weights_path):
            print(f"\ní•™ìŠµ ê²°ê³¼ í´ë”: {results_path}")
            weight_files = glob.glob(os.path.join(weights_path, '*.pt'))
            for weight_file in weight_files:
                size = os.path.getsize(weight_file) / (1024*1024)
                print(f"  ğŸ“ {os.path.basename(weight_file)} ({size:.1f}MB)")
            
            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ í™•ì¸
            checkpoints = glob.glob(os.path.join(weights_path, 'epoch*.pt'))
            if checkpoints:
                print(f"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ìˆ˜: {len(checkpoints)}ê°œ")
                
            # args.yaml íŒŒì¼ í™•ì¸ (ì›ë˜ í•™ìŠµ ì„¤ì •)
            args_file = os.path.join(results_path, 'args.yaml')
            if os.path.exists(args_file):
                print(f"  âš™ï¸  ì›ë˜ í•™ìŠµ ì„¤ì • íŒŒì¼ ì¡´ì¬: args.yaml")
                try:
                    with open(args_file, 'r') as f:
                        args = yaml.safe_load(f)
                    print(f"     - ì›ë˜ ë°ì´í„°ì…‹: {args.get('data')}")
                    print(f"     - ì›ë˜ ì—í¬í¬: {args.get('epochs')}")
                    print(f"     - ì›ë˜ ë°°ì¹˜ í¬ê¸°: {args.get('batch')}")
                except:
                    print(f"     (ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨)")
    else:
        print(f"\nâŒ í•™ìŠµ ê²°ê³¼ í´ë” ì—†ìŒ: {results_path}")

def resume_training(yaml_path, project_name, experiment_name, resume_from=None):
    """í•™ìŠµ ì¬ê°œ (ë°ì´í„°ì…‹ ê²€ì¦ í¬í•¨)"""
    
    # ë°ì´í„°ì…‹ ê²€ì¦
    if not validate_dataset_structure(DATASET_ROOT):
        print("âŒ ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨")
        return None
    
    # YAML íŒŒì¼ í™•ì¸/ìƒì„±
    if not os.path.exists(yaml_path):
        print(f"âŒ YAML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {yaml_path}")
        print("dataset.yaml íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        return None
    
    if resume_from is None:
        # ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ í•™ìŠµ ì¬ê°œ
        results_path = os.path.join(project_name, experiment_name)
        last_model = os.path.join(results_path, 'weights', 'last.pt')
        
        if os.path.exists(last_model):
            print(f"ğŸ”„ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ: {last_model}")
            model = YOLO(last_model)
        else:
            print(f"âŒ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {last_model}")
            print("ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            model = YOLO('yolo11l.pt')
    else:
        # íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
        if os.path.exists(resume_from):
            print(f"ğŸ”„ ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ: {resume_from}")
            model = YOLO(resume_from)
        else:
            print(f"âŒ ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {resume_from}")
            return None
    
    # í•™ìŠµ ì¬ê°œ (resume=Trueê°€ í•µì‹¬!)
    results = model.train(
        data=yaml_path,
        epochs=1000,                  # ì´ ëª©í‘œ ì—í¬í¬ (ì´ë¯¸ ì§„í–‰ëœ ì—í¬í¬ + ì¶”ê°€ ì—í¬í¬)
        imgsz=640,
        batch=16,
        device='0' if torch.cuda.is_available() else 'cpu',
        workers=4,
        project=project_name,
        name=experiment_name,
        resume=True,                  # ğŸ”¥ í•™ìŠµ ì¬ê°œ ì˜µì…˜
        save=True,
        save_period=10,
        patience=20,
        amp=True,
        lr0=0.01,
        weight_decay=0.0005,
        warmup_epochs=3,
        hsv_h=0.015,
        hsv_s=0.7,
        hsv_v=0.4,
        degrees=0.0,
        translate=0.1,
        scale=0.5,
        shear=0.0,
        flipud=0.0,
        fliplr=0.5,
        mosaic=1.0,
        mixup=0.0,
    )
    
    return results

def continue_training_from_best(yaml_path, project_name, experiment_name, additional_epochs=100):
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì—ì„œ ì¶”ê°€ í•™ìŠµ (ë°ì´í„°ì…‹ ê²€ì¦ í¬í•¨)"""
    
    # ë°ì´í„°ì…‹ ê²€ì¦
    if not validate_dataset_structure(DATASET_ROOT):
        print("âŒ ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨")
        return None
    
    results_path = os.path.join(project_name, experiment_name)
    best_model = os.path.join(results_path, 'weights', 'best.pt')
    
    if not os.path.exists(best_model):
        print(f"âŒ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_model}")
        return None
    
    print(f"ğŸš€ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì—ì„œ ì¶”ê°€ í•™ìŠµ ì‹œì‘: {best_model}")
    
    # ìƒˆë¡œìš´ ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ
    new_experiment_name = f"{experiment_name}_continued"
    
    model = YOLO(best_model)
    
    results = model.train(
        data=yaml_path,
        epochs=additional_epochs,     # ì¶”ê°€ë¡œ í•™ìŠµí•  ì—í¬í¬ ìˆ˜
        imgsz=640,
        batch=16,
        device='0' if torch.cuda.is_available() else 'cpu',
        workers=4,
        project=project_name,
        name=new_experiment_name,
        resume=False,                 # ìƒˆë¡œìš´ í•™ìŠµì´ë¯€ë¡œ False
        save=True,
        save_period=10,
        patience=20,
        amp=True,
        # ì¶”ê°€ í•™ìŠµì‹œì—ëŠ” ë” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
        lr0=0.001,                    # ì›ë˜ë³´ë‹¤ ë‚®ì€ í•™ìŠµë¥ 
        weight_decay=0.0005,
        warmup_epochs=1,              # ë” ì§§ì€ warmup
        # ë°ì´í„° ì¦ê°•ë„ ì¤„ì„
        hsv_h=0.01,
        hsv_s=0.5,
        hsv_v=0.3,
        degrees=0.0,
        translate=0.05,
        scale=0.3,
        shear=0.0,
        flipud=0.0,
        fliplr=0.5,
        mosaic=0.5,                   # ëª¨ìì´í¬ ì¦ê°• ì¤„ì„
        mixup=0.0,
    )
    
    return results

def safe_resume_with_setup():
    """ì•ˆì „í•œ í•™ìŠµ ì¬ê°œ (ëª¨ë“  ì„¤ì • ìë™ ì²˜ë¦¬)"""
    
    print("=== ì•ˆì „í•œ YOLO í•™ìŠµ ì¬ê°œ ===\n")
    
    # 1. ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦
    if not validate_dataset_structure(DATASET_ROOT):
        print("ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ë¨¼ì € ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
        return None
    
    # 2. dataset.yaml ìƒì„±/í™•ì¸
    yaml_path = create_or_check_dataset_yaml(DATASET_ROOT, CLASS_NAMES)
    
    # 3. ê¸°ì¡´ ëª¨ë¸ë“¤ í™•ì¸
    check_existing_models()
    
    # 4. ì‚¬ìš©ì ì„ íƒì— ë”°ë¥¸ ì‹¤í–‰
    print(f"\n=== ì‹¤í–‰ ì˜µì…˜ ===")
    print("1. ì¤‘ë‹¨ëœ í•™ìŠµ ì¬ê°œ (last.ptì—ì„œ)")
    print("2. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì—ì„œ ì¶”ê°€ í•™ìŠµ (best.ptì—ì„œ)")
    print("3. ìƒˆë¡œ ì‹œì‘")
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
    
    if choice == "1":
        return resume_training(yaml_path, PROJECT_NAME, EXPERIMENT_NAME)
    elif choice == "2":
        epochs = input("ì¶”ê°€ í•™ìŠµí•  ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ 100): ").strip()
        epochs = int(epochs) if epochs.isdigit() else 100
        return continue_training_from_best(yaml_path, PROJECT_NAME, EXPERIMENT_NAME, epochs)
    elif choice == "3":
        model = YOLO('yolo11l.pt')
        return model.train(
            data=yaml_path,
            epochs=500,
            imgsz=640,
            batch=16,
            device='0' if torch.cuda.is_available() else 'cpu',
            project=PROJECT_NAME,
            name=f"{EXPERIMENT_NAME}_new",
            resume=False
        )
    else:
        print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return None

def list_all_checkpoints(project_name, experiment_name):
    """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥"""
    results_path = os.path.join(project_name, experiment_name)
    weights_path = os.path.join(results_path, 'weights')
    
    if not os.path.exists(weights_path):
        print(f"âŒ ê°€ì¤‘ì¹˜ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {weights_path}")
        return []
    
    checkpoints = []
    
    # ê¸°ë³¸ ëª¨ë¸ë“¤
    for model_name in ['best.pt', 'last.pt']:
        model_path = os.path.join(weights_path, model_name)
        if os.path.exists(model_path):
            size = os.path.getsize(model_path) / (1024*1024)
            checkpoints.append((model_path, model_name, size))
    
    # ì—í¬í¬ë³„ ì²´í¬í¬ì¸íŠ¸ë“¤
    epoch_checkpoints = glob.glob(os.path.join(weights_path, 'epoch*.pt'))
    for checkpoint in sorted(epoch_checkpoints):
        name = os.path.basename(checkpoint)
        size = os.path.getsize(checkpoint) / (1024*1024)
        checkpoints.append((checkpoint, name, size))
    
    print(f"=== ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ({len(checkpoints)}ê°œ) ===")
    for i, (path, name, size) in enumerate(checkpoints):
        print(f"{i+1:2d}. {name:<15} ({size:6.1f}MB) - {path}")
    
    return checkpoints

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ì•ˆì „í•œ í•™ìŠµ ì¬ê°œ ì‹¤í–‰
    results = safe_resume_with_setup()
    
    if results:
        print("\nâœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ í•™ìŠµ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")